{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load only the Python subset from CodeSearchNet\ndataset = load_dataset(\"code_search_net\", \"python\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# Check if GPU is available, otherwise use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the model and move it to the appropriate device\nmodel = model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset[\"train\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"include_metadata = [\"func_documentation_string\", \"func_code_string\", \"func_name\", \"func_code_url\"]\ndf = pd.DataFrame(dataset)[include_metadata]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lightly process the func_code_string value. Remove trailing whitespaces and extra lines\nimport re\ndef preprocess_func_code_string(code):\n    code = \"\\n\".join([line.rstrip() for line in code.splitlines()])\n    code = re.sub(r'n\\{2,}', '\\n\\n', code)\n    return code","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[:5][\"func_code_string\"][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"func_code_string\"].apply(preprocess_func_code_string)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[:5][\"func_code_string\"][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loading CodeBERT\n!pip install transformers\n\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\nmodel = AutoModel.from_pretrained(\"microsoft/codebert-base\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tokenizing and appending raw tokens to the dataframe. \ndef tokenize_func_code_string(code):\n    return tokenizer.encode(code, truncation=True, padding=\"max_length\") #for consistent token length and code too long for tokenizer\n\ndf[\"func_code_string_token\"] = df[\"func_code_string\"].apply(tokenize_func_code_string)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Converting the token ids to tensor for pytorch, models like CodeBERT are built to process data in tensor form. Tensor form can be imagined as an \n#enhanced version of an array that supports operations on both CPU and GPU.\ndef to_tensor(token_ids):\n    return torch.tensor([token_ids]).to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"input_ids_tensor\"] = df[\"func_code_string_token\"].apply(to_tensor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nembeddings = [] #embeddings list\n\n#to ensure model is in eval mode\nmodel.eval()\n\n#disabling gradient computation since we aren't training the data. Gradients are only necessary while we are training a model, where the model is allowed to adjust its parameters\n#Also by disabling it we save memory and computing speed\nwith torch.no_grad():\n    for tensor in df[\"input_ids_tensor\"]:\n        tensor = tensor.unsqueeze(0).to(model.device) \n        \n        outputs = model(input_ids=tensor) \n        \n        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy() #Extracting CLS token \n        \n        embeddings.append(cls_embedding)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install numpy\n!pip install faiss-gpu\n\n#normalizing embedding will ensure all vectors lie on surface of a unit sphere so cosine similarity is applicable.Without normalization, original magnitudes of embedding will\n#be retained and l2/euclidean distance is more applicable. \n\n#I require capturing semantic similarity for my project as its a code retrieval system so will be normalizing the embedding. \n\nimport numpy as np\nfrom sklearn.preprocessing import normalize\nimport faiss\n\nembedding_dim = len(embeddings[0])\nembeddings_np = np.array(embeddings).astype('float32')\n\n#normalizing\nembeddings_np = normalize(embeddings, axis=1)\n\n#assign FAISS index\nindex = faiss.indexFlatIP(embedding_dim)\n\n#add normalized embeddingg\nindex.add(embeddings_np)\n\nprint(f\"No of embeddings indexed: {index.ntotal}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}